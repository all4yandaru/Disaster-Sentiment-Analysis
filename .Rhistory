predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- 3 # diusahakan ganjil
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
predict_knn$text
result_predict <- result_predict %>%
rbind(id = predict_knn$id, keyword = sentiment_predict)
}
result_predict
glimpse(bencana)
result_predict <- data.frame(id = integer(), keyword = character(), text = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- 3 # diusahakan ganjil
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
predict_knn$text
result_predict <- result_predict %>%
rbind(data.frame(id = predict_knn$id, keyword = sentiment_predict))
}
glimpse(bencana)
result_predict <- data.frame(id = integer(), keyword = character(), text = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- 3 # diusahakan ganjil
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
predict_knn$text
result_predict <- result_predict %>%
rbind(data.frame(id = predict_knn$id, keyword = sentiment_predict, text = predict_knn$text))
}
result_predict
result_predict
compare_predict <- testing
compare_predict
compare_predict <- inner_join(result_predict, by = "id")
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id")
compare_predict
compare_predict <- compare_predict %>%
left_join(result_predict, by = "id")
compare_predict
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id")
compare_predict
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id")
compare_predict
compare_predict <- testing
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id")
compare_predict
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id")
compare_predict
compare_predict <- testing
compare_predict
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id")
compare_predict
compare_predict <- testing
compare_predict
result_predict %>% arrange(desc(id))
compare_predict %>% arrange(desc(id))
sentiment_predict
cosine
result_predict %>% arrange(desc(id))
predict_knn$id
predict_knn$text
predict_knn
glimpse(bencana)
result_predict <- data.frame(id = integer(), keyword = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- 3 # diusahakan ganjil
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
result_predict <- result_predict %>%
rbind(data.frame(id = predict$id, keyword = sentiment_predict))
}
result_predict %>% arrange(desc(id))
compare_predict <- testing
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id")
compare_predict %>% arrange(desc(id))
testing
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id") %>%
mutate(interpretation = ifelse(keyword.x == keyword.y, 1, 0))
compare_predict %>% arrange(desc(id))
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id") %>%
mutate(akurasi_poin = ifelse(keyword.x == keyword.y, 1, 0))
compare_predict %>% arrange(desc(id))
akurasi <- compare_predict %>%
filter(akurasi_poin == 1) %>%
count()
akurasi
akurasi <- compare_predict %>%
filter(akurasi_poin == 1) %>%
count()$n / compare_predict %>%
count()
akurasi <- compare_predict %>%
filter(akurasi_poin == 1) %>%
count()$n / compare_predict %>%
count()$n
akurasi <- (compare_predict %>%
filter(akurasi_poin == 1) %>%
count())$n / (compare_predict %>%
count())$n
akurasi
test_akurasi < data.frame(key = integer(), akurasi = numeric())
test_akurasi < data.frame(key = integer(), akurasi = numeric())
test_akurasi <- data.frame(key = integer(), akurasi = numeric())
test_akurasi <- data.frame(k = integer(), akurasi = numeric())
glimpse(bencana)
test_akurasi <- data.frame(k = integer(), akurasi = numeric())
for (key in seq(3, 10, 2)) { # increment 3 - 10 sebanyak 2
result_predict <- data.frame(id = integer(), keyword = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- key # diusahakan ganjil
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
result_predict <- result_predict %>%
rbind(data.frame(id = predict$id, keyword = sentiment_predict))
}
result_predict %>% arrange(desc(id))
glimpse(bencana)
test_akurasi <- data.frame(k = integer(), akurasi = numeric())
for (key in seq(3, 10, 2)) { # increment 3 - 10 sebanyak 2
result_predict <- data.frame(id = integer(), keyword = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- key # diusahakan ganjil
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
result_predict <- result_predict %>%
rbind(data.frame(id = predict$id, keyword = sentiment_predict))
}
result_predict %>% arrange(desc(id))
# Testing akurasi
compare_predict <- testing
compare_predict <- compare_predict %>%
inner_join(result_predict, by = "id") %>%
mutate(akurasi_poin = ifelse(keyword.x == keyword.y, 1, 0))
compare_predict %>% arrange(desc(id))
akurasi <- (compare_predict %>%
filter(akurasi_poin == 1) %>%
count())$n / (compare_predict %>%
count())$n
akurasi
test_akurasi <- test_akurasi %>% rbind(data.frame(k = key, akurasi = akurasi))
}
tes
test_akurasi
compare_predict %>% arrange(desc(id))
train
compare_predict %>% arrange(desc(id))
glimpse(bencana)
#test_akurasi <- data.frame(k = integer(), akurasi = numeric())
#for (key in seq(3, 10, 2)) { # increment 3 - 10 sebanyak 2
hasil_prediksi <- data.frame(id = integer(), keyword = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- 7 # diusahakan ganjil, 7 yang paling tinggi
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
hasil_prediksi <- hasil_prediksi %>%
rbind(data.frame(id = predict$id, keyword = sentiment_predict))
}
hasil_prediksi %>% arrange(desc(id))
# Testing akurasi
compare_predict <- testing
compare_predict <- compare_predict %>%
inner_join(hasil_prediksi, by = "id") %>%
mutate(akurasi_poin = ifelse(keyword.x == keyword.y, 1, 0))
compare_predict %>% arrange(desc(id))
#akurasi <- (compare_predict %>%
#  filter(akurasi_poin == 1) %>%
#  count())$n / (compare_predict %>%
#  count())$n
#akurasi
#test_akurasi <- test_akurasi %>% rbind(data.frame(k = key, akurasi = akurasi))
#}
#test_akurasi
compare_predict %>% count()
compare_predict %>% count() %>% group_by(keyword.x)
compare_predict %>% group_by(keyword.x) %>% count()
knitr::opts_chunk$set(echo = TRUE)
test_akurasi
knitr::opts_chunk$set(echo = TRUE)
bencana <- bencana_raw %>% select(-location, -target) %>% filter(keyword == "flood" | keyword == "earthquake" | keyword == "volcano")
bencana_raw <- vroom(here("data-raw", "tweets.csv"))
library(shiny) # pembuatan web
library(here) # akses lokasi
library(vroom) # akses csv
library(dplyr) # manupulasi data glimpse, summaries, group, dll
library(ggplot2) # plotting
library(plotly) # plotting
library(topicmodels) # pembuatan LDA
library(tidyverse)
library(tidytext) # Untuk Text Mining dan Preprocessing
bencana_raw <- vroom(here("data-raw", "tweets.csv"))
glimpse(bencana_raw)
bencana <- bencana_raw %>% select(-location, -target) %>% filter(keyword == "flood" | keyword == "earthquake" | keyword == "volcano")
bencana
