# Testing akurasi
#compare_predict <- testing
#compare_predict <- compare_predict %>%
#  inner_join(hasil_prediksi, by = "id") %>%
#  mutate(akurasi_poin = ifelse(keyword.x == keyword.y, 1, 0))
#compare_predict %>% arrange(desc(id))
#compare_predict %>% group_by(keyword.x) %>% count()
#akurasi <- (compare_predict %>%
#  filter(akurasi_poin == 1) %>%
#  count())$n / (compare_predict %>%
#  count())$n
#akurasi
#test_akurasi <- test_akurasi %>% rbind(data.frame(k = key, akurasi = akurasi))
test_akurasi
#}
#test_akurasi
# penentuan training dan sample
train <- bencana
# penentuan training dan sample
train <- bencana
dt <- sort(sample(nrow(bencana_scraping), nrow(bencana_scraping)*.1)) # 10 persen data scraping
dt <- bencana_scraping[dt,]
view(dt)
testing <- data.frame(id = integer(), keyword = character(), text = character())
testing <- testing%>% rbind(data.frame(dt)) %>% mutate(keyword = NA)
view(testing)
#dt <- sort(sample(nrow(bencana), nrow(bencana)*.7)) # 70 persen utk training
#train <- bencana[dt,]
#testing <- bencana[-dt,]
glimpse(train)
glimpse(testing)
data_akhir <- hasil_prediksi %>%
left_join(bencana_scraping, by = "id") %>%
select(-keyword.y)
View(bencana_scraping)
data_akhir <- hasil_prediksi %>%
left_join(bencana_raw_scraping, by = "id") %>%
select(-keyword.y)
data_akhir <- hasil_prediksi %>%
left_join(bencana_raw_scraping, by = "id")
view(data_akhir)
data_akhir <- hasil_prediksi %>%
left_join(testing, by = "id") %>%
select(-keyword.y)
view(data_akhir)
data_akhir <- hasil_prediksi %>%
left_join(testing, by = "id") %>%
select(-keyword.y)
view(data_akhir)
glimpse(bencana)
#test_akurasi <- data.frame(k = integer(), akurasi = numeric())
#for (key in seq(3, 10, 2)) { # increment 3 - 10 sebanyak 2
hasil_prediksi <- data.frame(id = integer(), keyword = character(), text = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training untuk dimasukkan ke rumus cosine
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
# pembobotan, penjumlahan bobot
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- 7 # diusahakan ganjil, 7 yang paling tinggi
# dicari 7 data terdekat
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
hasil_prediksi <- hasil_prediksi %>%
rbind(data.frame(id = predict$id, keyword = sentiment_predict))
}
hasil_prediksi %>% mutate(text = testing$text) %>% arrange(desc(id))
# penentuan training dan sample
train <- bencana
dt <- sort(sample(nrow(bencana_scraping), nrow(bencana_scraping)*.1)) # 10 persen data scraping
dt <- bencana_scraping[dt,]
testing <- data.frame(id = integer(), keyword = character(), text = character())
testing <- testing%>% rbind(data.frame(dt)) %>% mutate(keyword = NA)
view(testing)
#dt <- sort(sample(nrow(bencana), nrow(bencana)*.7)) # 70 persen utk training
#train <- bencana[dt,]
#testing <- bencana[-dt,]
glimpse(train)
glimpse(testing)
# penentuan training dan sample
train <- bencana
dt <- sort(sample(nrow(bencana_scraping), nrow(bencana_scraping)*.1)) # 10 persen data scraping
dt <- bencana_scraping[dt,]
testing <- data.frame(id = integer(), keyword = character(), text = character())
testing <- testing%>% rbind(data.frame(dt)) %>% mutate(keyword = NA)
view(testing)
#dt <- sort(sample(nrow(bencana), nrow(bencana)*.7)) # 70 persen utk training
#train <- bencana[dt,]
#testing <- bencana[-dt,]
glimpse(train)
glimpse(testing)
view(testing)
glimpse(bencana)
#test_akurasi <- data.frame(k = integer(), akurasi = numeric())
#for (key in seq(3, 10, 2)) { # increment 3 - 10 sebanyak 2
hasil_prediksi <- data.frame(id = integer(), keyword = character(), text = character())
for (i in 1:nrow(testing)) {
cat(sprintf("Proses: (%d / %d)\n", i, nrow(testing)))
# mengambil data yang akan diprediksi kemudian digabungkan ke training
predict <- testing[i,]
data_tidy <- train %>% rbind(predict)
# membuat tf idf
data_tfidf <- data_tidy %>%
unnest_tokens(word, text) %>%
count(id, word, sort = TRUE) %>%
bind_tf_idf(word, id, n)
# mengambil kata2 yang akan diprediksi, kemudian mengalikan data predict dan data training untuk dimasukkan ke rumus cosine
predict_w <- data_tfidf %>% filter(id == predict$id)
bobot_training <- data.frame(id = integer(), total = numeric())
# pembobotan, penjumlahan bobot
for (j in 1:nrow(train)) {
temp_train <- data_tfidf %>%
filter(id == train$id[j])
data_join <- predict_w %>%
inner_join(temp_train, by = "word") %>%
mutate(hasil_kali = tf_idf.x * tf_idf.y)
bobot_training <- bobot_training %>%
rbind(data.frame(id = train$id[j], total = sum(data_join$hasil_kali)))
}
# menentukan panjang data
panjang_training <- data.frame(id = integer(), akar = numeric())
for (j in 1:nrow(data_tidy)) {
temp_train <- data_tfidf %>%
filter(id == data_tidy$id[j])
kuadrat_temp <- temp_train$tf_idf^2
akar_temp <- sqrt(sum(kuadrat_temp))
panjang_training <- panjang_training %>%
rbind(data.frame(id = data_tidy$id[j], akar = akar_temp))
}
# cosine similarity
predict_panjang <- panjang_training %>% filter(id == predict$id)
cosine <- data.frame(id = integer(), hasil_cosine = numeric())
for (j in 1:nrow(train)) {
cosine_temp <- bobot_training$total[j] / (predict_panjang$akar * panjang_training$akar[j])
cosine <- cosine %>%
rbind(data.frame(id = train$id[j], hasil_cosine = cosine_temp))
}
# sorting hasil cosine
cosine <- cosine %>% arrange(desc(hasil_cosine))
# proses knn
k <- 7 # diusahakan ganjil, 7 yang paling tinggi
# dicari 7 data terdekat
predict_knn <- cosine %>%
inner_join(train, by = "id") %>%
select(id, hasil_cosine, keyword, text) %>%
head(k)
sentiment_predict <- predict_knn %>% count(keyword)
sentiment_predict <- sentiment_predict$keyword[which.max(sentiment_predict$n)]
hasil_prediksi <- hasil_prediksi %>%
rbind(data.frame(id = predict$id, keyword = sentiment_predict))
}
hasil_prediksi %>% mutate(text = testing$text) %>% arrange(desc(id))
data_akhir <- hasil_prediksi %>%
left_join(testing, by = "id") %>%
select(-keyword.y)
view(data_akhir)
# Testing akurasi
#compare_predict <- testing
#compare_predict <- compare_predict %>%
#  inner_join(hasil_prediksi, by = "id") %>%
#  mutate(akurasi_poin = ifelse(keyword.x == keyword.y, 1, 0))
#compare_predict %>% arrange(desc(id))
#compare_predict %>% group_by(keyword.x) %>% count()
#akurasi <- (compare_predict %>%
#  filter(akurasi_poin == 1) %>%
#  count())$n / (compare_predict %>%
#  count())$n
#akurasi
#test_akurasi <- test_akurasi %>% rbind(data.frame(k = key, akurasi = akurasi))
test_akurasi
#}
#test_akurasi
text_raw <- bencana_raw_scraping %>% select(id, text)
data_akhir <- hasil_prediksi %>%
left_join(text_raw, by = "id") %>%
select(-keyword.y)
data_akhir <- hasil_prediksi %>%
left_join(text_raw, by = "id")
view(data_akhir)
View(bencana_scraping)
View(bencana_raw_scraping)
View(bencanaraw)
View(bencana_scraping)
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(syuzhet)
library(lubridate)
library(scales)
library(reshape2)
# melihat isi terlebih dahulu
corpus <- iconv(testing$text)
corpus <- Corpus(VectorSource(corpus))
library(tm)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# melihat isi terlebih dahulu
corpus <- iconv(testing$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Term document matrix
tdm <- TermDocumentMatrix(corpus)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20] # melihat matrix banyaknya kata-i pada kalimat-j
# Bar plot
w <- rowSums(tdm)
w <- subset(w, w>=25) # melihat banyak kata yang lebih dari 25
w # melihat banyaknya frekuensi kata
barplot(w,
las = 2,
col = rainbow(4))
w # melihat banyaknya frekuensi kata
inspect(corpus[1:5])
tdm[1:10, 1:20] # melihat matrix banyaknya kata-i pada kalimat-j
barplot(w,
las = 2,
col = rainbow(4))
barplot(w,
las = 2,
col = rainbow(4))
#library(shiny) # pembuatan web
library(here) # akses lokasi
library(vroom) # akses csv
library(dplyr) # manupulasi data glimpse, summaries, group, dll
library(ggplot2) # plotting
#library(plotly) # plotting
#library(topicmodels) # pembuatan LDA
library(tidyverse)
library(tidytext)
library(wordcloud)
library(syuzhet)
library(lubridate)
library(scales)
library(reshape2)
bencana_raw <- vroom(here("data-raw", "tweets.csv"))
glimpse(bencana_raw)
#Build Corpus
library(tm) # library text mining
bencana_raw
bencana_clean <- bencana_raw
# cleaning bencana
bencana_clean$text <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", bencana_clean$text)
bencana_clean$text <- gsub("@\\w+", " ", bencana_clean$text)
bencana_clean$text <- gsub("#\\w+", " ", bencana_clean$text)
bencana_clean$text <- gsub("https://t.co/\\w+", " ", bencana_clean$text)
bencana_clean$text <- gsub('[[:punct:]]', " ", bencana_clean$text)
bencana_clean$text <- gsub('[[:cntrl:]]', " ", bencana_clean$text)
bencana_clean$text <- gsub('\\d+', "", bencana_clean$text)
bencana_clean$text <- gsub("[ \t]{2,}", " ", bencana_clean$text)
bencana_clean$text <- gsub("^\\s+|\\s+$", "", bencana_clean$text)
bencana_clean$text <- gsub("[^\x01-\x7F]", "", bencana_clean$text)
bencana_clean$text <- tolower(bencana_clean$text)
bencana_clean <- bencana_clean %>%
group_by(id, keyword) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
summarize(text = str_c(word, collapse = " ")) %>%
ungroup()
view(bencana_clean)
# melihat isi terlebih dahulu
corpus <- iconv(bencana_clean$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
corpus <- tm_map(corpus, removeWords, c('amp'))
# Term document matrix
tdm <- TermDocumentMatrix(corpus)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20] # melihat matrix banyaknya kata-i pada kalimat-j
# Bar plot
w <- rowSums(tdm)
w <- subset(w, w>=25) # melihat banyak kata yang lebih dari 25
w # melihat banyaknya frekuensi kata
barplot(w,
las = 2,
col = rainbow(4))
# Word Cloud
w <- sort(rowSums(tdm), decreasing = TRUE)
w
set.seed(1234)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
w_data <- data.frame(names(w), w)
colnames(w_data) <- c('word', 'freq')
# Sentiment analysis
tweets <- iconv(bencana_clean$text)
s <- get_nrc_sentiment(tweets)
head(s)
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores for Disasters')
w # melihat banyaknya frekuensi kata
# Word Cloud
w <- sort(rowSums(tdm), decreasing = TRUE)
w
set.seed(1234)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
w_data <- data.frame(names(w), w)
colnames(w_data) <- c('word', 'freq')
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
library(wordcloud)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
set.seed(1234)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
w_data <- data.frame(names(w), w)
colnames(w_data) <- c('word', 'freq')
w_data
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
# Sentiment analysis
tweets <- iconv(testing$text)
s <- get_nrc_sentiment(tweets)
head(s)
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores for Disasters')
# Sentiment analysis
tweets <- iconv(testing$text)
s <- get_nrc_sentiment(tweets)
head(s)
barplot(colSums(s),
las = 2,
col = rainbow(10),
ylab = 'Count',
main = 'Sentiment Scores for Disasters')
# melihat isi terlebih dahulu
corpus <- iconv(testing$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Term document matrix
tdm <- TermDocumentMatrix(corpus)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20] # melihat matrix banyaknya kata-i pada kalimat-j
# Bar plot
w <- rowSums(tdm)
w <- subset(w, w>=25) # melihat banyak kata yang lebih dari 25
w # melihat banyaknya frekuensi kata
# Word Cloud
w <- sort(rowSums(tdm), decreasing = TRUE)
w
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
w_data <- data.frame(names(w), w)
colnames(w_data) <- c('word', 'freq')
# melihat isi terlebih dahulu
corpus <- iconv(testing$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Term document matrix
tdm <- TermDocumentMatrix(corpus)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20] # melihat matrix banyaknya kata-i pada kalimat-j
# Bar plot
w <- rowSums(tdm)
w <- subset(w, w>=25) # melihat banyak kata yang lebih dari 25
#w # melihat banyaknya frekuensi kata
# Word Cloud
w <- sort(rowSums(tdm), decreasing = TRUE)
w
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
# melihat isi terlebih dahulu
corpus <- iconv(testing$text)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Term document matrix
tdm <- TermDocumentMatrix(corpus)
tdm <- as.matrix(tdm)
tdm[1:10, 1:20] # melihat matrix banyaknya kata-i pada kalimat-j
# Bar plot
w <- rowSums(tdm)
w <- subset(w, w>=25) # melihat banyak kata yang lebih dari 25
#w # melihat banyaknya frekuensi kata
# Word Cloud
w <- sort(rowSums(tdm), decreasing = TRUE)
#w
set.seed(222)
wordcloud(words = names(w),
freq = w,
max.words = 500,
random.order = F,
min.freq = 50,
colors = brewer.pal(8, 'Dark2'),
scale = c(2, 0.3))
w_data <- data.frame(names(w), w)
colnames(w_data) <- c('word', 'freq')
